{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe41c838",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor\n",
    "\n",
    "model_name = \"Qwen/Qwen3-VL-4B-Instruct\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3966f6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"video\",\n",
    "                \"video\": \"test_video.mp4\",\n",
    "                \"min_pixels\": 4 * 32 * 32,\n",
    "                \"max_pixels\": 256 * 32 * 32,\n",
    "                \"total_pixels\": 20480 * 32 * 32,\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe ball movements in great details.\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "images, videos, video_kwargs = process_vision_info(messages, image_patch_size=16, return_video_kwargs=True, return_video_metadata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "043447b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eadc1f31e1ce4544810547cd20603cc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen3VLForConditionalGeneration(\n",
      "  (model): Qwen3VLModel(\n",
      "    (visual): Qwen3VLVisionModel(\n",
      "      (patch_embed): Qwen3VLVisionPatchEmbed(\n",
      "        (proj): Conv3d(3, 1024, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n",
      "      )\n",
      "      (pos_embed): Embedding(2304, 1024)\n",
      "      (rotary_pos_emb): Qwen3VLVisionRotaryEmbedding()\n",
      "      (blocks): ModuleList(\n",
      "        (0-23): 24 x Qwen3VLVisionBlock(\n",
      "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Qwen3VLVisionAttention(\n",
      "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (mlp): Qwen3VLVisionMLP(\n",
      "            (linear_fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (linear_fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (act_fn): GELUTanh()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (merger): Qwen3VLVisionPatchMerger(\n",
      "        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (linear_fc1): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "        (act_fn): GELU(approximate='none')\n",
      "        (linear_fc2): Linear(in_features=4096, out_features=2560, bias=True)\n",
      "      )\n",
      "      (deepstack_merger_list): ModuleList(\n",
      "        (0-2): 3 x Qwen3VLVisionPatchMerger(\n",
      "          (norm): LayerNorm((4096,), eps=1e-06, elementwise_affine=True)\n",
      "          (linear_fc1): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "          (act_fn): GELU(approximate='none')\n",
      "          (linear_fc2): Linear(in_features=4096, out_features=2560, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (language_model): Qwen3VLTextModel(\n",
      "      (embed_tokens): Embedding(151936, 2560)\n",
      "      (layers): ModuleList(\n",
      "        (0-35): 36 x Qwen3VLTextDecoderLayer(\n",
      "          (self_attn): Qwen3VLTextAttention(\n",
      "            (q_proj): Linear(in_features=2560, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "            (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "            (o_proj): Linear(in_features=4096, out_features=2560, bias=False)\n",
      "            (q_norm): Qwen3VLTextRMSNorm((128,), eps=1e-06)\n",
      "            (k_norm): Qwen3VLTextRMSNorm((128,), eps=1e-06)\n",
      "          )\n",
      "          (mlp): Qwen3VLTextMLP(\n",
      "            (gate_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
      "            (up_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
      "            (down_proj): Linear(in_features=9728, out_features=2560, bias=False)\n",
      "            (act_fn): SiLUActivation()\n",
      "          )\n",
      "          (input_layernorm): Qwen3VLTextRMSNorm((2560,), eps=1e-06)\n",
      "          (post_attention_layernorm): Qwen3VLTextRMSNorm((2560,), eps=1e-06)\n",
      "        )\n",
      "      )\n",
      "      (norm): Qwen3VLTextRMSNorm((2560,), eps=1e-06)\n",
      "      (rotary_emb): Qwen3VLTextRotaryEmbedding()\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen3VLForConditionalGeneration\n",
    "\n",
    "model = Qwen3VLForConditionalGeneration.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=\"bfloat16\",\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cab4b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the videos and according metadatas\n",
    "if videos is not None:\n",
    "    videos, video_metadatas = zip(*videos)\n",
    "    videos, video_metadatas = list(videos), list(video_metadatas)\n",
    "else:\n",
    "    video_metadatas = None\n",
    "\n",
    "# since qwen-vl-utils has resize the images/videos, \\\n",
    "# we should pass do_resize=False to avoid duplicate operation in processor!\n",
    "inputs = processor(text=text, images=images, videos=videos, video_metadata=video_metadatas, return_tensors=\"pt\", do_resize=False, **video_kwargs)\n",
    "inputs = inputs.to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205512e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25019cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(output_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6e725e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def index_to_letter(index):\n",
    "    assert 0 <= index < 26, f\"Index must be between 0 and 25, got {index}\"\n",
    "    return chr(index + ord(\"A\"))\n",
    "\n",
    "def letter_to_index(letter):\n",
    "    assert len(letter) == 1 and letter.isalpha(), f\"Letter must be a single alphabetic character, got {letter}\"\n",
    "    return ord(letter.upper()) - ord(\"A\")\n",
    "\n",
    "def get_assistant_mask(input_ids):\n",
    "    # Vectorized search for sequence \"<|im_start|>assistant\\n\" -> [151644, 77091, 198]\n",
    "    pattern = torch.tensor([151644, 77091, 198], device=input_ids.device, dtype=input_ids.dtype)\n",
    "    k = pattern.numel()\n",
    "    batch_size, seq_len = input_ids.shape\n",
    "\n",
    "    if seq_len < k:\n",
    "        return torch.zeros_like(input_ids, dtype=torch.bool)\n",
    "\n",
    "    # Create all sliding windows of length k: shape (B, T-k+1, k)\n",
    "    windows = input_ids.unfold(1, k, 1)\n",
    "    # Match windows against the pattern\n",
    "    matches = (windows == pattern).all(dim=-1)  # (B, T-k+1)\n",
    "\n",
    "    any_match = matches.any(dim=1)\n",
    "    # First occurrence index (undefined if no match, so guard with any_match)\n",
    "    first_pos = torch.where(\n",
    "        any_match,\n",
    "        matches.int().argmax(dim=1),\n",
    "        torch.full((batch_size,), seq_len, device=input_ids.device, dtype=torch.long),\n",
    "    )\n",
    "\n",
    "    start_after = first_pos + k\n",
    "    mask = torch.arange(seq_len, device=input_ids.device).unsqueeze(0) >= start_after.unsqueeze(1)\n",
    "    return mask\n",
    "\n",
    "def video_assistant_data_collator(samples):\n",
    "    conversations = []\n",
    "    for sample in samples:\n",
    "        question = sample[\"question\"]\n",
    "        choices = sample[\"choices\"]\n",
    "        \n",
    "        question_prompt = f\"{question}\\n\\n\"\n",
    "        for i, choice in enumerate(choices):\n",
    "            question_prompt += f\"{index_to_letter(i)}. {choice}\\n\"\n",
    "        \n",
    "        ground_truth_indices = sample[\"ground_truth\"]  # List[int] indicies\n",
    "        ground_truth_answer = \"\".join(index_to_letter(i) for i in ground_truth_indices)\n",
    "        \n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"video\",\n",
    "                        \"video\": sample[\"video\"],\n",
    "                        \"min_pixels\": 4 * 32 * 32,\n",
    "                        \"max_pixels\": 256 * 32 * 32,\n",
    "                        \"total_pixels\": 20480 * 32 * 32,\n",
    "                    },\n",
    "                    {\"type\": \"text\", \"text\": question_prompt},\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": ground_truth_answer,\n",
    "            }\n",
    "        ]\n",
    "        conversations.append(conversation)\n",
    "    \n",
    "    text = processor.apply_chat_template(conversations, tokenize=False, add_generation_prompt=False)\n",
    "    images, videos, video_kwargs = process_vision_info(conversations, image_patch_size=16, return_video_kwargs=True, return_video_metadata=True)\n",
    "\n",
    "    # split the videos and according metadatas\n",
    "    if videos is not None:\n",
    "        videos, video_metadatas = zip(*videos)\n",
    "        videos, video_metadatas = list(videos), list(video_metadatas)\n",
    "    else:\n",
    "        video_metadatas = None\n",
    "\n",
    "    # since qwen-vl-utils has resize the images/videos,\n",
    "    # we should pass do_resize=False to avoid duplicate operation in processor!\n",
    "    inputs = processor(text=text, images=images, videos=videos, video_metadata=video_metadatas, return_tensors=\"pt\", do_resize=False, **video_kwargs)\n",
    "\n",
    "    input_ids = inputs.input_ids\n",
    "    assistant_mask = get_assistant_mask(input_ids)\n",
    "    # TODO: there's a bug here, the assistant mask doesn't mask the turn end token\n",
    "    \n",
    "    labels = inputs.input_ids.clone()\n",
    "    labels[~assistant_mask] = -100\n",
    "    \n",
    "    return {\n",
    "        **inputs,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
