{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe41c838",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor\n",
    "\n",
    "model_name = \"Qwen/Qwen3-VL-4B-Instruct\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e75e29ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1867, 151643],\n",
       "        [  1706,     35],\n",
       "        [    34, 151643]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = processor.tokenizer\n",
    "\n",
    "tokenizer([\"AB\", \"ACD\", \"C\"], add_special_tokens=False, padding=True, return_tensors=\"pt\")[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "619f1535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([0, 3, 1, 1])\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scottc/links/scratch/causal_pool/.venv/lib/python3.12/site-packages/torchvision/io/_video_deprecation_warning.py:9: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torchvision.io import read_video\n",
    "\n",
    "video_path = \"space_woaudio.mp4\"\n",
    "video, audio, info = read_video(video_path, output_format=\"TCHW\")\n",
    "\n",
    "print(video.shape)  # frames, channels, height, width\n",
    "print(info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3966f6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qwen-vl-utils using torchvision to read video.\n",
      "/home/scottc/links/scratch/causal_pool/.venv/lib/python3.12/site-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
      "  warnings.warn(\n",
      "video_reader_backend torchvision error, use torchvision as default, msg: 'video_fps'\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'video_fps'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/links/scratch/causal_pool/.venv/lib/python3.12/site-packages/qwen_vl_utils/vision_process.py:411\u001b[39m, in \u001b[36mfetch_video\u001b[39m\u001b[34m(ele, image_patch_size, return_video_sample_fps, return_video_metadata)\u001b[39m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m411\u001b[39m     video, video_metadata, sample_fps = \u001b[43mVIDEO_READER_BACKENDS\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvideo_reader_backend\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mele\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/links/scratch/causal_pool/.venv/lib/python3.12/site-packages/qwen_vl_utils/vision_process.py:212\u001b[39m, in \u001b[36m_read_video_torchvision\u001b[39m\u001b[34m(ele)\u001b[39m\n\u001b[32m    205\u001b[39m video, audio, info = io.read_video(\n\u001b[32m    206\u001b[39m     video_path,\n\u001b[32m    207\u001b[39m     start_pts=ele.get(\u001b[33m\"\u001b[39m\u001b[33mvideo_start\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m0.0\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m    210\u001b[39m     output_format=\u001b[33m\"\u001b[39m\u001b[33mTCHW\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    211\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m total_frames, video_fps = video.size(\u001b[32m0\u001b[39m), \u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvideo_fps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    213\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtorchvision:  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_path\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_frames\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_fps\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m, time=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime.time()\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mst\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: 'video_fps'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mqwen_vl_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m process_vision_info\n\u001b[32m     19\u001b[39m text = processor.apply_chat_template(\n\u001b[32m     20\u001b[39m     messages, tokenize=\u001b[38;5;28;01mFalse\u001b[39;00m, add_generation_prompt=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     21\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m images, videos, video_kwargs = \u001b[43mprocess_vision_info\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_patch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_video_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_video_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     24\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/links/scratch/causal_pool/.venv/lib/python3.12/site-packages/qwen_vl_utils/vision_process.py:517\u001b[39m, in \u001b[36mprocess_vision_info\u001b[39m\u001b[34m(conversations, return_video_kwargs, return_video_metadata, image_patch_size)\u001b[39m\n\u001b[32m    515\u001b[39m     image_inputs.append(fetch_image(vision_info, image_patch_size=image_patch_size))\n\u001b[32m    516\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mvideo\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m vision_info:\n\u001b[32m--> \u001b[39m\u001b[32m517\u001b[39m     video_input, video_sample_fps = \u001b[43mfetch_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvision_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_video_sample_fps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m                \u001b[49m\u001b[43mimage_patch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_patch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_video_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_video_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    519\u001b[39m     video_sample_fps_list.append(video_sample_fps)\n\u001b[32m    520\u001b[39m     video_inputs.append(video_input)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/links/scratch/causal_pool/.venv/lib/python3.12/site-packages/qwen_vl_utils/vision_process.py:414\u001b[39m, in \u001b[36mfetch_video\u001b[39m\u001b[34m(ele, image_patch_size, return_video_sample_fps, return_video_metadata)\u001b[39m\n\u001b[32m    412\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    413\u001b[39m         logger.warning(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mvideo_reader_backend \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_reader_backend\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m error, use torchvision as default, msg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m414\u001b[39m         video, video_metadata, sample_fps = \u001b[43mVIDEO_READER_BACKENDS\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtorchvision\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mele\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    416\u001b[39m     \u001b[38;5;66;03m# The input is a list of frames\u001b[39;00m\n\u001b[32m    417\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ele[\u001b[33m\"\u001b[39m\u001b[33mvideo\u001b[39m\u001b[33m\"\u001b[39m], (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/links/scratch/causal_pool/.venv/lib/python3.12/site-packages/qwen_vl_utils/vision_process.py:212\u001b[39m, in \u001b[36m_read_video_torchvision\u001b[39m\u001b[34m(ele)\u001b[39m\n\u001b[32m    204\u001b[39m st = time.time()\n\u001b[32m    205\u001b[39m video, audio, info = io.read_video(\n\u001b[32m    206\u001b[39m     video_path,\n\u001b[32m    207\u001b[39m     start_pts=ele.get(\u001b[33m\"\u001b[39m\u001b[33mvideo_start\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m0.0\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m    210\u001b[39m     output_format=\u001b[33m\"\u001b[39m\u001b[33mTCHW\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    211\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m total_frames, video_fps = video.size(\u001b[32m0\u001b[39m), \u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvideo_fps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    213\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtorchvision:  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_path\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_frames\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_fps\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m, time=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime.time()\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mst\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    214\u001b[39m nframes = smart_nframes(ele, total_frames=total_frames, video_fps=video_fps)\n",
      "\u001b[31mKeyError\u001b[39m: 'video_fps'"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"video\",\n",
    "                \"video\": \"test_video.mp4\",\n",
    "                \"min_pixels\": 4 * 32 * 32,\n",
    "                \"max_pixels\": 256 * 32 * 32,\n",
    "                \"total_pixels\": 20480 * 32 * 32,\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe ball movements in great details.\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "images, videos, video_kwargs = process_vision_info(\n",
    "    messages, image_patch_size=16, return_video_kwargs=True, return_video_metadata=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "043447b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eadc1f31e1ce4544810547cd20603cc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen3VLForConditionalGeneration(\n",
      "  (model): Qwen3VLModel(\n",
      "    (visual): Qwen3VLVisionModel(\n",
      "      (patch_embed): Qwen3VLVisionPatchEmbed(\n",
      "        (proj): Conv3d(3, 1024, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n",
      "      )\n",
      "      (pos_embed): Embedding(2304, 1024)\n",
      "      (rotary_pos_emb): Qwen3VLVisionRotaryEmbedding()\n",
      "      (blocks): ModuleList(\n",
      "        (0-23): 24 x Qwen3VLVisionBlock(\n",
      "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Qwen3VLVisionAttention(\n",
      "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (mlp): Qwen3VLVisionMLP(\n",
      "            (linear_fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (linear_fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (act_fn): GELUTanh()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (merger): Qwen3VLVisionPatchMerger(\n",
      "        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (linear_fc1): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "        (act_fn): GELU(approximate='none')\n",
      "        (linear_fc2): Linear(in_features=4096, out_features=2560, bias=True)\n",
      "      )\n",
      "      (deepstack_merger_list): ModuleList(\n",
      "        (0-2): 3 x Qwen3VLVisionPatchMerger(\n",
      "          (norm): LayerNorm((4096,), eps=1e-06, elementwise_affine=True)\n",
      "          (linear_fc1): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "          (act_fn): GELU(approximate='none')\n",
      "          (linear_fc2): Linear(in_features=4096, out_features=2560, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (language_model): Qwen3VLTextModel(\n",
      "      (embed_tokens): Embedding(151936, 2560)\n",
      "      (layers): ModuleList(\n",
      "        (0-35): 36 x Qwen3VLTextDecoderLayer(\n",
      "          (self_attn): Qwen3VLTextAttention(\n",
      "            (q_proj): Linear(in_features=2560, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "            (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "            (o_proj): Linear(in_features=4096, out_features=2560, bias=False)\n",
      "            (q_norm): Qwen3VLTextRMSNorm((128,), eps=1e-06)\n",
      "            (k_norm): Qwen3VLTextRMSNorm((128,), eps=1e-06)\n",
      "          )\n",
      "          (mlp): Qwen3VLTextMLP(\n",
      "            (gate_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
      "            (up_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
      "            (down_proj): Linear(in_features=9728, out_features=2560, bias=False)\n",
      "            (act_fn): SiLUActivation()\n",
      "          )\n",
      "          (input_layernorm): Qwen3VLTextRMSNorm((2560,), eps=1e-06)\n",
      "          (post_attention_layernorm): Qwen3VLTextRMSNorm((2560,), eps=1e-06)\n",
      "        )\n",
      "      )\n",
      "      (norm): Qwen3VLTextRMSNorm((2560,), eps=1e-06)\n",
      "      (rotary_emb): Qwen3VLTextRotaryEmbedding()\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen3VLForConditionalGeneration\n",
    "\n",
    "model = Qwen3VLForConditionalGeneration.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=\"bfloat16\",\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cab4b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the videos and according metadatas\n",
    "if videos is not None:\n",
    "    videos, video_metadatas = zip(*videos)\n",
    "    videos, video_metadatas = list(videos), list(video_metadatas)\n",
    "else:\n",
    "    video_metadatas = None\n",
    "\n",
    "# since qwen-vl-utils has resize the images/videos, \\\n",
    "# we should pass do_resize=False to avoid duplicate operation in processor!\n",
    "inputs = processor(text=text, images=images, videos=videos, video_metadata=video_metadatas, return_tensors=\"pt\", do_resize=False, **video_kwargs)\n",
    "inputs = inputs.to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205512e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25019cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(output_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6e725e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def index_to_letter(index):\n",
    "    assert 0 <= index < 26, f\"Index must be between 0 and 25, got {index}\"\n",
    "    return chr(index + ord(\"A\"))\n",
    "\n",
    "def letter_to_index(letter):\n",
    "    assert len(letter) == 1 and letter.isalpha(), f\"Letter must be a single alphabetic character, got {letter}\"\n",
    "    return ord(letter.upper()) - ord(\"A\")\n",
    "\n",
    "def get_assistant_mask(input_ids):\n",
    "    # Vectorized search for sequence \"<|im_start|>assistant\\n\" -> [151644, 77091, 198]\n",
    "    pattern = torch.tensor([151644, 77091, 198], device=input_ids.device, dtype=input_ids.dtype)\n",
    "    k = pattern.numel()\n",
    "    batch_size, seq_len = input_ids.shape\n",
    "\n",
    "    if seq_len < k:\n",
    "        return torch.zeros_like(input_ids, dtype=torch.bool)\n",
    "\n",
    "    # Create all sliding windows of length k: shape (B, T-k+1, k)\n",
    "    windows = input_ids.unfold(1, k, 1)\n",
    "    # Match windows against the pattern\n",
    "    matches = (windows == pattern).all(dim=-1)  # (B, T-k+1)\n",
    "\n",
    "    any_match = matches.any(dim=1)\n",
    "    # First occurrence index (undefined if no match, so guard with any_match)\n",
    "    first_pos = torch.where(\n",
    "        any_match,\n",
    "        matches.int().argmax(dim=1),\n",
    "        torch.full((batch_size,), seq_len, device=input_ids.device, dtype=torch.long),\n",
    "    )\n",
    "\n",
    "    start_after = first_pos + k\n",
    "    mask = torch.arange(seq_len, device=input_ids.device).unsqueeze(0) >= start_after.unsqueeze(1)\n",
    "    return mask\n",
    "\n",
    "def video_assistant_data_collator(samples):\n",
    "    conversations = []\n",
    "    for sample in samples:\n",
    "        question = sample[\"question\"]\n",
    "        choices = sample[\"choices\"]\n",
    "        \n",
    "        question_prompt = f\"{question}\\n\\n\"\n",
    "        for i, choice in enumerate(choices):\n",
    "            question_prompt += f\"{index_to_letter(i)}. {choice}\\n\"\n",
    "        \n",
    "        ground_truth_indices = sample[\"ground_truth\"]  # List[int] indicies\n",
    "        ground_truth_answer = \"\".join(index_to_letter(i) for i in ground_truth_indices)\n",
    "        \n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"video\",\n",
    "                        \"video\": sample[\"video\"],\n",
    "                        \"min_pixels\": 4 * 32 * 32,\n",
    "                        \"max_pixels\": 256 * 32 * 32,\n",
    "                        \"total_pixels\": 20480 * 32 * 32,\n",
    "                    },\n",
    "                    {\"type\": \"text\", \"text\": question_prompt},\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": ground_truth_answer,\n",
    "            }\n",
    "        ]\n",
    "        conversations.append(conversation)\n",
    "    \n",
    "    text = processor.apply_chat_template(conversations, tokenize=False, add_generation_prompt=False)\n",
    "    images, videos, video_kwargs = process_vision_info(conversations, image_patch_size=16, return_video_kwargs=True, return_video_metadata=True)\n",
    "\n",
    "    # split the videos and according metadatas\n",
    "    if videos is not None:\n",
    "        videos, video_metadatas = zip(*videos)\n",
    "        videos, video_metadatas = list(videos), list(video_metadatas)\n",
    "    else:\n",
    "        video_metadatas = None\n",
    "\n",
    "    # since qwen-vl-utils has resize the images/videos,\n",
    "    # we should pass do_resize=False to avoid duplicate operation in processor!\n",
    "    inputs = processor(text=text, images=images, videos=videos, video_metadata=video_metadatas, return_tensors=\"pt\", do_resize=False, **video_kwargs)\n",
    "\n",
    "    input_ids = inputs.input_ids\n",
    "    assistant_mask = get_assistant_mask(input_ids)\n",
    "    # TODO: there's a bug here, the assistant mask doesn't mask the turn end token\n",
    "    \n",
    "    labels = inputs.input_ids.clone()\n",
    "    labels[~assistant_mask] = -100\n",
    "    \n",
    "    return {\n",
    "        **inputs,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
